# Deep Research Agent Evaluation

æœ¬ç›®å½•åŒ…å«ç”¨äºè¯„æµ‹ Agentica Agent å¤šè½®æ·±åº¦æœç´¢ç ”ç©¶èƒ½åŠ›çš„è„šæœ¬å’Œæ•°æ®é›†ã€‚

## æ¦‚è¿°

è¯„æµ‹åŸºäº `enable_multi_round=True` çš„å¤šè½®ç­–ç•¥ï¼ŒAgent ä¼šè‡ªåŠ¨è¿›è¡Œå¤šè½®æœç´¢ã€è®¿é—®ç½‘é¡µã€åˆ†æä¿¡æ¯ï¼Œç›´åˆ°æ‰¾åˆ°ç­”æ¡ˆã€‚

### å¤šè½®ç­–ç•¥å·¥ä½œåŸç†

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Multi-Round Strategy                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. ç”¨æˆ·æé—®                                                  â”‚
â”‚  2. Agent è°ƒç”¨æœç´¢å·¥å…·è·å–ç›¸å…³ç½‘é¡µ                              â”‚
â”‚  3. Agent è®¿é—®ç½‘é¡µè·å–è¯¦ç»†ä¿¡æ¯                                  â”‚
â”‚  4. Agent åˆ†æä¿¡æ¯ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦ç»§ç»­æœç´¢                         â”‚
â”‚  5. é‡å¤ 2-4 ç›´åˆ°æ— å·¥å…·è°ƒç”¨ï¼ˆä»»åŠ¡å®Œæˆï¼‰                          â”‚
â”‚  6. è¿”å›æœ€ç»ˆç­”æ¡ˆ                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## å¿«é€Ÿå¼€å§‹

```bash
# åŸºç¡€è¯„æµ‹ï¼ˆ3ä¸ªæ ·æœ¬ï¼‰
python run.py --model gpt-4o --dataset browsecomp_zh_small --eval_n_limit 3

# å®Œæ•´è¯„æµ‹
python run.py --model gpt-4o --dataset browsecomp_zh_small --eval_n_limit 0

# ä½¿ç”¨ä¸åŒæ¨¡å‹
python run.py --model deepseek-reasoner --dataset browsecomp_zh_small

# è°ƒæ•´å¤šè½®å‚æ•°
python run.py --model gpt-4o --max_rounds 100 --max_tokens 128000
```

## å‘½ä»¤è¡Œå‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `--model` | str | `gpt-4o` | æ¨¡å‹ ID |
| `--dataset` | str | `browsecomp_zh_small` | è¯„æµ‹æ•°æ®é›† |
| `--eval_n_limit` | int | `3` | è¯„æµ‹æ ·æœ¬æ•°ï¼ˆ0=å…¨éƒ¨ï¼‰ |
| `--max_rounds` | int | `20` | æœ€å¤§è½®æ¬¡ |
| `--max_tokens` | int | `40000` | Token ä¸Šé™ |
| `--tools` | str | `baidu` | æœç´¢å·¥å…·ï¼ˆbaidu/serper/jina/allï¼‰ |
| `--debug` | int | `0` | è°ƒè¯•æ¨¡å¼ï¼ˆ0=å…³ï¼Œ1=å¼€ï¼‰ |
| `--output_dir` | str | `outputs` | è¾“å‡ºç›®å½• |
| `--skip_judge` | flag | - | è·³è¿‡ LLM è¯„åˆ¤ |

## æ”¯æŒçš„æ•°æ®é›†

| æ•°æ®é›† | æè¿° | æ ·æœ¬æ•° |
|--------|------|--------|
| `browsecomp_zh_small` | BrowseComp ä¸­æ–‡å°è§„æ¨¡ | ~10 |
| `browsecomp_zh` | BrowseComp ä¸­æ–‡å®Œæ•´ | ~100 |
| `browsecomp_en_small` | BrowseComp è‹±æ–‡å°è§„æ¨¡ | ~10 |
| `browsecomp_en` | BrowseComp è‹±æ–‡å®Œæ•´ | ~100 |
| `simple_qa` | SimpleQA é—®ç­” | - |
| `gaia_2023_all_validation` | GAIA 2023 éªŒè¯é›† | ~165 |
| `xbench_deepsearch` | XBench æ·±åº¦æœç´¢ | - |
| `sailorfog-QA` | SailorFog QA | - |

## è¾“å‡ºæ–‡ä»¶

è¯„æµ‹å®Œæˆåä¼šåœ¨ `outputs/` ç›®å½•ç”Ÿæˆï¼š

1. **predictions-{dataset}.jsonl** - é¢„æµ‹ç»“æœ
   ```json
   {
     "question": "é—®é¢˜å†…å®¹",
     "answer": "æ ‡å‡†ç­”æ¡ˆ",
     "prediction": "æ¨¡å‹é¢„æµ‹",
     "messages": [...],
     "tool_calls": [...],
     "full_response": "å®Œæ•´å“åº”"
   }
   ```

2. **summary-{dataset}.json** - è¯„æµ‹æ‘˜è¦
   ```json
   {
     "dataset": "browsecomp_zh_small",
     "model": "gpt-4o",
     "accuracy": 66.67,
     "correct": 2,
     "total": 3,
     "statistics": {
       "avg_tool_calls": 8.5,
       "avg_rounds": 4.2,
       ...
     }
   }
   ```

## è¯„æµ‹æŒ‡æ ‡

- **Accuracy**: æ­£ç¡®ç‡ï¼ˆç”± LLM Judge åˆ¤æ–­ï¼‰
- **Avg Tool Calls**: å¹³å‡å·¥å…·è°ƒç”¨æ¬¡æ•°
- **Avg Rounds**: å¹³å‡å¯¹è¯è½®æ¬¡
- **Avg Answer Length**: å¹³å‡ç­”æ¡ˆé•¿åº¦
- **Avg Reasoning Length**: å¹³å‡æ¨ç†é•¿åº¦

## æœç´¢å·¥å…·é…ç½®

| é…ç½® | å·¥å…· | è¯´æ˜ |
|------|------|------|
| `baidu` | BaiduSearchTool + UrlCrawlerTool | ç™¾åº¦æœç´¢ï¼ˆä¸­æ–‡æ¨èï¼‰ |
| `serper` | SearchSerperTool + UrlCrawlerTool | Serper APIï¼ˆéœ€è¦ API Keyï¼‰ |
| `jina` | JinaTool + UrlCrawlerTool | Jina AIï¼ˆéœ€è¦ API Keyï¼‰ |
| `all` | å…¨éƒ¨å·¥å…· | æ‰€æœ‰æœç´¢å·¥å…· |

## ç¤ºä¾‹è¾“å‡º

```
============================================================
ğŸ“Š EVALUATION RESULTS
============================================================
Dataset: browsecomp_zh_small
Model: gpt-4o
Instances: 3
----------------------------------------
âœ… Accuracy: 66.67% (2/3)
ğŸ“ˆ Avg Tool Calls: 8.5
   - Search: 3.2
   - Visit: 5.3
   - Other: 0.0
ğŸ“ Avg Rounds: 4.2 (max: 8)
ğŸ“„ Avg Answer Length: 156
ğŸ§  Avg Reasoning Length: 2340
============================================================
```

## ç›®å½•ç»“æ„

```
evaluation/
â”œâ”€â”€ README.md           # æœ¬æ–‡æ¡£
â”œâ”€â”€ run.py              # è¯„æµ‹è„šæœ¬
â”œâ”€â”€ prompt.py           # Judge æç¤ºè¯
â”œâ”€â”€ data/               # è¯„æµ‹æ•°æ®é›†
â”‚   â”œâ”€â”€ browsecomp_zh_small.jsonl
â”‚   â”œâ”€â”€ browsecomp_zh.jsonl
â”‚   â”œâ”€â”€ browsecomp_en_small.jsonl
â”‚   â”œâ”€â”€ gaia_2023_all_validation.jsonl
â”‚   â””â”€â”€ ...
â””â”€â”€ outputs/            # è¯„æµ‹ç»“æœ
    â”œâ”€â”€ predictions-*.jsonl
    â””â”€â”€ summary-*.json
```

## ç›¸å…³æ–‡æ¡£

- [Multi-Round Deep Research Agent](../docs/multi_round_deep_research_agent.md) - å¤šè½®ç­–ç•¥å®ç°åŸç†
